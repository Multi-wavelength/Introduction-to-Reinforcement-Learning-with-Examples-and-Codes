{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 7 Variations of Policy Gradient Algorithms\n",
        "\n",
        "This notebook was created by Kellen Kanarios (kellenkk@umich.edu).\n",
        "\n",
        "## Example: Reinforcement Learning with Human Feedback (RLHF) for Positive Movie Reviews\n",
        "\n",
        "In this notebook, we will use proximal policy optimization (PPO) to finetune a large language model (LLM) for more positive movie reviews.\n",
        "\n",
        "### Problem Description\n",
        "\n",
        "The IMDB dataset contains 50k movie review annotated with \"positive\"/\"negative\" feedback indicating the sentiment. There are some language models that have been pretrained on this dataset to generate movie reviews. We will consider two models, \"edbeeching/gpt2-medium-imdb\" and \"lvwerra/gpt2-imdb\" that are available in the Huggingface model zoo [1]. The generated movie reviews could be either positive or negative. Our goal is to finetune these language models to generate more positive movie reviews.\n",
        "\n",
        "In an LLM, each output is generated by appending one token at a time to the input sequence. A token is essentially a word or a sub-word. We are then given whether the total response is acceptable. This sounds like an RL problem!\n",
        "\n",
        "### Formulation\n",
        "\n",
        "To formulate language generation as an RL problem, we must first construct an MDP.\n",
        "\n",
        "- *State* $s$:\n",
        "    \n",
        "    The state $s$ is the current sequence of words, which is also a sequence of tokens.\n",
        "\n",
        "- *Action $u$*:\n",
        "\n",
        "    The action $u$ is the token that will be appended to the end of the sequence. Taking an action is just appending the token to the end of the state (current sequence). $\\pi(u | s)$ is the probability of saying token $u$ given sentence $s$.\n",
        "\n",
        "- *Transition*:\n",
        "\n",
        "    Suppose we take action $u_t$ in state $s_t$. Then the next state $s_{t+1}=s_t + u_t$, which means appending $u_t$ to the end of $s_t$.\n",
        "\n",
        "- *Reward*:\n",
        "    \n",
        "    At the end of the trajectory, a reward will be given for the total trajectory. We will use a classifier to analyze the sentiment of the produced sentences and use the classifier's outputs as rewards signals. We will use the classifier \"lvwerra/distilbert-imdb\" in [2].\n",
        "\n",
        "- *Objective*: Maximize the expected reward.\n",
        "\n",
        "\n",
        "We can train (finetune) the language model using clipped PPO (along with a few more KL tricks). Namely, we want\n",
        "\n",
        "\\begin{align*}\n",
        "    \\max_{\\tilde{w}} \\mathbb{E}_{x\\sim \\rho_w, u \\sim \\pi_w}\\left[\\min\\left\\{\\frac{\\pi_{\\tilde{w}}(u | x)}{\\pi_{w}(u | x)}A_{w}(x, u), \\left(\\frac{\\pi_{\\tilde{w}}(u | x)}{\\pi_{w}(u | x)}\\right)^{1+\\epsilon}_{1-\\epsilon} A_{w}(x, u) \\right\\}\\right],\n",
        "\\end{align*}\n",
        "where $(x)_a^b$ is the clipping function such that $(x)_a^b=x$ if $x\\in[a,b]$, $a$ if $x<a$, and $b$ if $x>b$.\n",
        "\n",
        "\n",
        "### References\n",
        "\n",
        "[1] https://huggingface.co/models\n",
        "\n",
        "[2] https://huggingface.co/lvwerra/distilbert-imdb\n",
        "\n",
        "[3] The following was used as starter code https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb\n"
      ],
      "metadata": {
        "id": "LYG9KKi8ji8o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install and Import Packages"
      ],
      "metadata": {
        "id": "zUq12lHIVeP5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4lA2RGmjdFd-"
      },
      "outputs": [],
      "source": [
        "!pip install  --upgrade \\\n",
        "  \"transformers\" \\\n",
        "  \"datasets\" \\\n",
        "  \"trl\" \\\n",
        "  \"peft\" \\\n",
        "  \"wandb\" \\\n",
        "  \"bitsandbytes\" \\\n",
        "  \"accelerate\" \\\n",
        "  \"optimum\" \\\n",
        "  \"pandas\"\n",
        "\n",
        "import torch\n",
        "import random\n",
        "import numpy\n",
        "\n",
        "import wandb\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from trl.core import LengthSampler\n",
        "from datasets import load_dataset\n",
        "\n",
        "from trl import PPOConfig\n",
        "from peft import LoraConfig\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "from trl import AutoModelForCausalLMWithValueHead, PPOTrainer\n",
        "import bitsandbytes as bnb\n",
        "from transformers import pipeline, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
        "from optimum.bettertransformer import BetterTransformer\n",
        "\n",
        "from tqdm import tqdm\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RztmlDvNTrVt"
      },
      "source": [
        "#Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFL6Glv3jxqk"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=0):\n",
        "    # set seed for all possible avenues of stochasticity\n",
        "    numpy.random.seed(seed=seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "set_seed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7wC9esp4i1n"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collator(data):\n",
        "    return dict((key, [d[key] for d in data]) for key in data[0])"
      ],
      "metadata": {
        "id": "Zh_BjMIhjOZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA8LhewFQzaz"
      },
      "source": [
        "#Setup Wandb\n",
        "To track training results, we will use wandb.\n",
        "**Please visit the site and make an account.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfUKYHaSQ5Kq"
      },
      "outputs": [],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pngacx2I1b11"
      },
      "source": [
        "# Choose a Model\n",
        "\n",
        "We will choose one of the following two language models with different Low-Rank Adaptation (LoRA) configurations. LoRA is a Parameter-Efficient Fine-Tuning (PEFT) method that decomposes a large matrix into two smaller low-rank matrices in the attention layers. This drastically reduces the number of parameters that need to be fine-tuned. See https://huggingface.co/docs/peft/en/package_reference/lora for details of LoRA.\n",
        "\n",
        "Model (a):\n",
        "*   Model name: \"edbeeching/gpt2-medium-imdb\" in https://huggingface.co/edbeeching/gpt2-medium-imdb\n",
        "*   LoRA condiguration: r=128, lora_alpha=256, lora_dropout=0.05.\n",
        "\n",
        "Model (b):\n",
        "*   Model name: \"lvwerra/gpt2-imdb\" in https://huggingface.co/lvwerra/gpt2-imdb\n",
        "*   LoRA condiguration: r=512, lora_alpha=1024, lora_dropout=0.05.\n",
        "\n",
        "Model (b) is smaller than Model (a).\n",
        "\n",
        "**Please set `model_chosen` to `\"a\"` or `\"b\"` in the following code block.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-SO5MyR57_Z"
      },
      "outputs": [],
      "source": [
        "# choose model \"a\" or \"b\"\n",
        "model_chosen = \"a\"  # or \"b\"\n",
        "\n",
        "if model_chosen == \"a\":\n",
        "    part_a = True\n",
        "    model_id = \"edbeeching/gpt2-medium-imdb\"\n",
        "elif model_chosen == \"b\":\n",
        "    part_a = False\n",
        "    model_id = \"lvwerra/gpt2-imdb\"\n",
        "else:\n",
        "    print(\"Wrong model input!\")\n",
        "    exit(1)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rt7tpTh_FF01"
      },
      "source": [
        "# Preprocess and Pretokenize Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiYv_m3_1aEM"
      },
      "outputs": [],
      "source": [
        "def build_dataset(\n",
        "    tokenizer,\n",
        "    dataset_name=\"imdb\",\n",
        "    input_min_text_length=8,\n",
        "    input_max_text_length=24,\n",
        "):\n",
        "    \"\"\"\n",
        "    Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
        "    customize this function to train the model on its own dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset_name (`str`):\n",
        "            The name of the dataset to be loaded.\n",
        "\n",
        "    Returns:\n",
        "        dataloader (`torch.utils.data.DataLoader`):\n",
        "            The dataloader for the dataset.\n",
        "    \"\"\"\n",
        "    train_dataset = load_dataset(dataset_name, split=\"train\")\n",
        "    original_columns = train_dataset.column_names\n",
        "\n",
        "    train_dataset = train_dataset.filter(lambda x: len(x[\"text\"]) > 200, batched=False)\n",
        "\n",
        "    def preprocess_function(samples):\n",
        "        new_samples = {\n",
        "            \"query\": [],\n",
        "            \"input_ids\": [],\n",
        "        }\n",
        "        for review in samples[\"text\"]:\n",
        "            input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
        "            query = review[: input_size()]\n",
        "            new_samples[\"query\"].append(query)\n",
        "            tokenized_query = tokenizer.encode(\n",
        "                query,\n",
        "            )\n",
        "            new_samples[\"input_ids\"].append(tokenized_query)\n",
        "\n",
        "        return new_samples\n",
        "\n",
        "    ds = train_dataset.map(\n",
        "        preprocess_function,\n",
        "        batched=True,\n",
        "        remove_columns=original_columns,\n",
        "    )\n",
        "\n",
        "    ds.set_format(type=\"torch\")\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8Xql-D79QlA"
      },
      "outputs": [],
      "source": [
        "dataset = build_dataset(tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzOHjgjyV5j8"
      },
      "source": [
        "# Setup the Model\n",
        "\n",
        "*   You can try different ``cliprange`` = 0.1, 0.2, 0.3 in the ``PPOConfig`` object.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGh5fU1gWcRg"
      },
      "outputs": [],
      "source": [
        "# Experiment with different cliprange\n",
        "\n",
        "config = PPOConfig(\n",
        "  model_name=model_id,\n",
        "  learning_rate=1.41e-5,\n",
        "  cliprange=0.2,\n",
        "  use_score_scaling=True,\n",
        "  use_score_norm=True,\n",
        "  score_clip=0.5,\n",
        "  log_with=\"wandb\"\n",
        ")\n",
        "\n",
        "peft_config_a = LoraConfig(\n",
        "    r=128,\n",
        "    lora_alpha=256,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    fan_in_fan_out=True,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "peft_config_b = LoraConfig(\n",
        "    r=512,\n",
        "    lora_alpha=1024,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    fan_in_fan_out=True,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qaxRBn-85Kt8"
      },
      "outputs": [],
      "source": [
        "peft_config = peft_config_a if part_a else peft_config_b\n",
        "\n",
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    peft_config=peft_config,\n",
        ")\n",
        "model = BetterTransformer.transform(model, keep_original_model=False)\n",
        "\n",
        "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    low_cpu_mem_usage=True,\n",
        ").eval()\n",
        "\n",
        "optimizer = bnb.optim.Adam8bit(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "ppo_trainer = PPOTrainer(\n",
        "    config=config,\n",
        "    model=model,\n",
        "    ref_model=ref_model,\n",
        "    tokenizer=tokenizer,\n",
        "    dataset=dataset,\n",
        "    data_collator=collator,\n",
        "    optimizer=optimizer,\n",
        ")\n",
        "\n",
        "device = ppo_trainer.accelerator.device\n",
        "if ppo_trainer.accelerator.num_processes == 1:\n",
        "    device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug\n",
        "\n",
        "reward_model_id = \"lvwerra/distilbert-imdb\"\n",
        "reward_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    reward_model_id,\n",
        "    max_length=512,\n",
        "    truncation=True,\n",
        ")\n",
        "reward_model = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=reward_model_id,\n",
        "    tokenizer=reward_tokenizer,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kF6EZdz4n8P"
      },
      "outputs": [],
      "source": [
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_gBNaxr7AdI"
      },
      "outputs": [],
      "source": [
        "generation_kwargs = {\n",
        "    \"min_length\": 4,\n",
        "    \"top_k\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True,\n",
        "    \"pad_token_id\": tokenizer.eos_token_id,\n",
        "    \"eos_token_id\": tokenizer.eos_token_id,\n",
        "    \"max_new_tokens\": 16,\n",
        "    \"remove_invalid_values\": True,\n",
        "    \"return_prompt\": False\n",
        "}\n",
        "\n",
        "sent_kwargs = {\"top_k\": None, \"function_to_apply\": \"none\", \"padding\": True, \"batch_size\": 16}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the Reward Model and Define the Reward Function"
      ],
      "metadata": {
        "id": "x6MQdPqMpDiF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tpicoEqRgCw"
      },
      "outputs": [],
      "source": [
        "good_text = \"this movie was really good!!\"\n",
        "print(reward_model(good_text, **sent_kwargs))\n",
        "\n",
        "bad_text = \"this movie was really bad!!\"\n",
        "print(reward_model(bad_text, **sent_kwargs))\n",
        "\n",
        "def reward(output):\n",
        "    return output[1][\"score\"] if output[1][\"label\"] == \"POSITIVE\" else output[0][\"score\"]\n",
        "\n",
        "print(reward(reward_model(good_text, **sent_kwargs)))\n",
        "print(reward(reward_model(bad_text, **sent_kwargs)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start Training"
      ],
      "metadata": {
        "id": "--F_-E5MpdMm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KbqkKxi72Gi"
      },
      "outputs": [],
      "source": [
        "%%wandb\n",
        "\n",
        "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
        "    query_tensors = batch[\"input_ids\"]\n",
        "\n",
        "    #### Generate response\n",
        "    response_tensors = ppo_trainer.generate(\n",
        "        query_tensors,\n",
        "        **generation_kwargs\n",
        "    )\n",
        "    batch[\"response\"] = tokenizer.batch_decode(response_tensors)\n",
        "\n",
        "    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
        "    reward_outputs = reward_model(texts, **sent_kwargs)\n",
        "    reward_model.call_count = 0\n",
        "    rewards = [torch.tensor(reward(output)) for output in reward_outputs]\n",
        "\n",
        "    #### Run PPO step\n",
        "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
        "    ppo_trainer.log_stats(stats, batch, rewards)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results and Comparison"
      ],
      "metadata": {
        "id": "G-tfW_zHtkIm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7lmV_nY_wmF"
      },
      "outputs": [],
      "source": [
        "output_min_length = 16\n",
        "output_max_length = 32\n",
        "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
        "#### get a batch from the dataset\n",
        "gen_kwargs = {\"min_length\": -1, \"top_k\": 0.0, \"top_p\": 1.0, \"do_sample\": True, \"pad_token_id\": tokenizer.eos_token_id, \"remove_invalid_values\": True}\n",
        "\n",
        "bs = 16\n",
        "game_data = dict()\n",
        "dataset.set_format(\"pandas\")\n",
        "df_batch = dataset[:].sample(bs)\n",
        "game_data[\"query\"] = df_batch[\"query\"].tolist()\n",
        "query_tensors = df_batch[\"input_ids\"].tolist()\n",
        "\n",
        "response_tensors_ref, response_tensors = [], []\n",
        "\n",
        "for i in range(bs):\n",
        "    gen_len = output_length_sampler()\n",
        "    output = ref_model.generate(\n",
        "        torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device), max_new_tokens=gen_len, **gen_kwargs\n",
        "    ).squeeze()[-gen_len:]\n",
        "    response_tensors_ref.append(output)\n",
        "    output = model.generate(\n",
        "        torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device), max_new_tokens=gen_len, **gen_kwargs\n",
        "    ).squeeze()[-gen_len:]\n",
        "    response_tensors.append(output)\n",
        "\n",
        "#### decode responses\n",
        "game_data[\"response (before)\"] = [tokenizer.decode(response_tensors_ref[i]) for i in range(bs)]\n",
        "game_data[\"response (after)\"] = [tokenizer.decode(response_tensors[i]) for i in range(bs)]\n",
        "\n",
        "#### sentiment analysis of query/response pairs before/after\n",
        "texts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (before)\"])]\n",
        "game_data[\"rewards (before)\"] = [reward(output) for output in reward_model(texts, **sent_kwargs)]\n",
        "\n",
        "texts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (after)\"])]\n",
        "game_data[\"rewards (after)\"] = [reward(output) for output in reward_model(texts, **sent_kwargs)]\n",
        "\n",
        "# store results in a dataframe\n",
        "df_results = pd.DataFrame(game_data)\n",
        "df_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVSj6B6rRty7"
      },
      "outputs": [],
      "source": [
        "print(\"mean:\")\n",
        "display(df_results[[\"rewards (before)\", \"rewards (after)\"]].mean())\n",
        "print()\n",
        "print(\"median:\")\n",
        "display(df_results[[\"rewards (before)\", \"rewards (after)\"]].median())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}