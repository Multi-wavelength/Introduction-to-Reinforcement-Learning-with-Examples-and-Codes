{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjm8m0WffM44"
      },
      "source": [
        "# Chapter 7 Variations of Policy Gradient Algorithms\n",
        "\n",
        "This notebook was created by Chenshun Ni (nichensh@umich.edu).\n",
        "\n",
        "## Example: Inverted Pendulum Swingup Problem - PPO-Clip and PPO-Penalty\n",
        "In this notebook, we will implement the PPO-Clip and PPO-Penalty algorithms and compare their performance on solving the 'Pendulum-v1' task from the OpenAI Gym.\n",
        "\n",
        "### Problem Description [1]\n",
        "\n",
        "The inverted pendulum swingup problem is based on the classic problem in control theory. The system consists of a pendulum attached at one end to a fixed point, and the other end being free. The pendulum starts in a random position and the goal is to apply torque on the free end to swing it into an upright position, with its center of gravity right above the fixed point.\n",
        "\n",
        "The diagram below specifies the coordinate system used for the implementation of the pendulumâ€™s dynamic equations.\n",
        "![Pendulum.png](data:image/png;base64,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)\n",
        "\n",
        "### Formulation [1]\n",
        "\n",
        "- *State* $x$:\n",
        "\n",
        "  The state is a ``ndarray`` with shape ``(3,)`` representing the x-y coordinates of the pendulumâ€™s free end and its angular velocity.\n",
        "\n",
        "  <table>\n",
        "    <tr>\n",
        "      <th>Num</th>\n",
        "      <th>Observation</th>\n",
        "      <th>Min</th>\n",
        "      <th>Max</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>0</td>\n",
        "      <td>x = cos(theta)</td>\n",
        "      <td>-1</td>\n",
        "      <td>1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>1</td>\n",
        "      <td>y = sin(theta)</td>\n",
        "      <td>-1</td>\n",
        "      <td>1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>2</td>\n",
        "      <td>Angular Velocity</td>\n",
        "      <td>-8</td>\n",
        "      <td>8</td>\n",
        "    </tr>\n",
        "  </table>\n",
        "\n",
        "  We can see that the state space is continuous.\n",
        "\n",
        "- *Action* $u$:\n",
        "\n",
        "  The action is a ``ndarray`` with shape ``(1,)`` representing the torque applied to free end of the pendulum.\n",
        "\n",
        "  <table>\n",
        "    <tr>\n",
        "      <th>Num</th>\n",
        "      <th>Action</th>\n",
        "      <th>Min</th>\n",
        "      <th>Max</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <th>Torque</th>\n",
        "      <th>-2</th>\n",
        "      <th>2</th>\n",
        "    </tr>\n",
        "  </table>\n",
        "\n",
        "  We can see that the action space is also continuous.\n",
        "\n",
        "- *Reward* $r$:\n",
        "\n",
        "  The reward function is defined as\n",
        "\n",
        "  $$\n",
        "  r = -(theta^2 + 0.1 * theta\\_dt^2 + 0.001 * torque^2)\n",
        "  $$\n",
        "  where $theta$ is the pendulumâ€™s angle normalized between $[-\\pi, \\pi]$ (with 0 being in the upright position), and $theta\\_dt$ is the angular velocity of the pendulumâ€™s free end. Based on the above equation, the minimum reward that can be obtained is $-(\\pi^2 + 0.1 * 8^2 + 0.001 * 2^2) = -16.2736044$, while the maximum reward is zero (pendulum is upright with zero velocity and no torque applied).\n",
        "\n",
        "- *Starting State*:\n",
        "\n",
        "  The starting state is a random angle in $[-\\pi, \\pi]$ and a random angular velocity in $[-1,1]$\n",
        "\n",
        "- *Episode Truncation*:\n",
        "  The episode truncates at 200 time steps.\n",
        "\n",
        "- *Objective*:\n",
        "\n",
        "  To swing the pendulum into an upright position, with its center of gravity right above the fixed point.\n",
        "\n",
        "\n",
        "### PPO Algorithms\n",
        "\n",
        "There are two forms of PPO: PPO-Penalty and PPO-Clip.\n",
        "\n",
        "#### PPO-Penalty\n",
        "\n",
        "The main idea of PPO-Penalty is to directly include the KL-divergence constraint into the objective function and convert the original problem into an unconstrained optimization problem, i.e.\n",
        "\n",
        "\\begin{align*}\n",
        "&w^*\\in \\arg\\max_{\\tilde{w}} \\mathbb{E}_{\\rho_{{w}}, u\\sim \\pi_{{w}_k}}\\left[\\frac{\\pi_{\\tilde{w}}(u|x)}{\\pi_{w_k}(u|x)} A_{w_k}(x,u)- \\beta \\hbox{KL}(\\pi_{\\tilde w}\\| \\pi_{w_k})\\right].\n",
        "\\end{align*}\n",
        "\n",
        "After obtaining $w^*,$ compute $d=\\mathbb{E}_{\\rho_w}\\left[\\hbox{KL}(\\pi_{w^*}\\| \\pi_{w_k})\\right].$\n",
        "Let $\\epsilon$ be a hyperparameter controlling the difference between the new policy and the old policy.\n",
        "If $d\\in (\\epsilon/1.5, 1.5\\epsilon),$ use the same $\\beta$ for the next policy update. Otherwise, adjust $\\beta$ such that if $d\\leq \\frac{\\epsilon}{1.5},$ $\\beta\\leftarrow \\beta/2,$ and if $d\\geq {1.5}{\\epsilon},$ $\\beta\\leftarrow 2\\beta$.\n",
        "\n",
        "#### PPO-Clip\n",
        "\n",
        "PPO-clip avoids the KL divergence term and considers the following modified objective.\n",
        "\n",
        "\\begin{align*}\n",
        "&\\max_{\\tilde{w}} \\mathbb{E}_{\\rho_{{w_k}}, u\\sim \\pi_{{w_k}}}\\left[\\min\\left\\{\\frac{\\pi_{\\tilde{w}}(u|x)}{\\pi_{w_k}(u|x)} A_{w_k}(x,u), \\left(\\frac{\\pi_{\\tilde{w}}(u|x)}{\\pi_{w_k}(u|x)}\\right)_{1-\\epsilon}^{1+\\epsilon} A_{w_k}(x,u)\\right\\} \\right]\n",
        "\\end{align*}\n",
        "\n",
        "where $\\{x\\}_a^b$ is the clipping function such that $\\{x\\}_a^b$ equals to $x$ if $x\\in[a,b],$ $a$ if $x<a$ and $b$ if $x>b.$\n",
        "Here $\\epsilon$ is a hyperparameter controlling the clip range.\n",
        "The intuition is that if $A_{w_k}(x,u)>0$ (which suggests that such state-action pair should be encouraged), the maximization operator will increase $\\frac{\\pi_{\\tilde{w}}(u|x)}{\\pi_{w_k}(u|x)}$ but it will not be larger than $1+\\epsilon$. On the contrary, if $A_{w_k}(x,u)<0$ (which suggests that such state-action pair should not be encouraged), the maximization operator will decrease $\\frac{\\pi_{\\tilde{w}}(u|x)}{\\pi_{w_k}(u|x)}$, but it will not be smaller than $1-\\epsilon$.\n",
        "\n",
        "\n",
        "### Reference\n",
        "\n",
        "[1] The problem description and formulation are from OpenAI Gym documentation: https://gymnasium.farama.org/environments/classic_control/pendulum/\n",
        "\n",
        "[2] Part of the code is adapted from CleanRL repository: https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo_continuous_action.py\n",
        "\n",
        "[3] Schulman, John, et al. \"Proximal policy optimization algorithms.\" arXiv preprint arXiv:1707.06347 (2017). https://arxiv.org/abs/1707.06347\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCEH7UCPJ3JH"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions.normal import Normal\n",
        "from collections import deque\n",
        "from statistics import mean\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRo7Ed4iM0Bc"
      },
      "source": [
        "### Define the Actor Network and the Critic Network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4igfEl_KImy"
      },
      "outputs": [],
      "source": [
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "    \"\"\"\n",
        "    This function initializes the weights of the neural network\n",
        "    \"\"\"\n",
        "    torch.nn.init.orthogonal_(layer.weight, std)\n",
        "    torch.nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
        "      \"\"\"\n",
        "      This class contains the critic and actor network\n",
        "      Args:\n",
        "        state_dim: state dimension\n",
        "        action_dim: action dimension\n",
        "        hidden_dim: number of neurons in one fully connected hidden layer\n",
        "      \"\"\"\n",
        "      super().__init__()\n",
        "      \"\"\"\n",
        "      Structure of critic network\n",
        "      \"\"\"\n",
        "      self.critic = nn.Sequential(\n",
        "          layer_init(nn.Linear(state_dim, hidden_dim)),\n",
        "          nn.ReLU(),\n",
        "          layer_init(nn.Linear(hidden_dim, hidden_dim)),\n",
        "          nn.ReLU(),\n",
        "          layer_init(nn.Linear(hidden_dim, 1), std=1.0),\n",
        "      )\n",
        "      \"\"\"\n",
        "      Structure of actor network\n",
        "      \"\"\"\n",
        "      self.actor_mean = nn.Sequential(\n",
        "          layer_init(nn.Linear(state_dim, hidden_dim)),\n",
        "          nn.ReLU(),\n",
        "          layer_init(nn.Linear(hidden_dim, hidden_dim)),\n",
        "          nn.ReLU(),\n",
        "          layer_init(nn.Linear(hidden_dim, action_dim), std=0.01),\n",
        "      )\n",
        "      self.actor_logstd = nn.Parameter(torch.zeros(1, action_dim)) #std's should be independent of means\n",
        "\n",
        "    def get_value(self, states):\n",
        "        \"\"\"\n",
        "        The forward process of the critic network that maps state(s) -> value(s)\n",
        "        Args:\n",
        "          states: torch.Tensor with shape (batch_size, state_dim)\n",
        "        Returns:\n",
        "          values: torch.Tensor with shape (batch_size, 1)\n",
        "        \"\"\"\n",
        "        values = self.critic(states)\n",
        "        return values\n",
        "\n",
        "    def get_mean_and_std(self, states):\n",
        "        \"\"\"\n",
        "        The forward process of the actor network that maps state(s) -> mean(s), std(s)\n",
        "        Args:\n",
        "          states: torch.Tensor with shape (batch_size, state_dim)\n",
        "        Returns:\n",
        "          action_means: torch.Tensor with shape (batch_size, action_dim)\n",
        "          action_std: torch.Tensor with shape (1, action_dim)\n",
        "        \"\"\"\n",
        "        action_means = self.actor_mean(states)\n",
        "        action_std = torch.exp(self.actor_logstd)\n",
        "        return action_means, action_std\n",
        "\n",
        "    def get_action_or_logprob(self, states, actions=None):\n",
        "        \"\"\"\n",
        "        The forward process of the actor network that maps state(s) -> action(s)\n",
        "                                      or (state(s),action(s)) -> log_prob(s)\n",
        "        Args:\n",
        "          states: torch.Tensor with shape (batch_size, state_dim)\n",
        "          actions: torch.Tensor with shape (batch_size, action_dim)\n",
        "        Returns:\n",
        "          actions: torch.Tensor with shape (batch_size, action_dim)\n",
        "          or\n",
        "          log_probs: torch.Tensor with shape (batch_size, )\n",
        "        \"\"\"\n",
        "        action_means = self.actor_mean(states)\n",
        "        action_std = torch.exp(self.actor_logstd)\n",
        "        probs = Normal(action_means, action_std + 1e-7)\n",
        "        if actions is None:\n",
        "            actions = probs.sample()\n",
        "            return actions\n",
        "        log_probs = probs.log_prob(actions).sum(1)\n",
        "        entropy = probs.entropy().sum(1)\n",
        "        return log_probs, entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDOJ_-dGgr1g"
      },
      "source": [
        "### Generalized Advantage Estimation (GAE) [4]\n",
        "Generalized Advantage Estimation (GAE) is a method used to estimate the advantage function $A$. Here we give a brief introduction of GAE.\n",
        "\n",
        "We denote the temporal difference as $\\delta_t=r_t+\\gamma V(x_{t+1})-V(x_t)$ where $V$ is the value function. Then we have\n",
        "$$\n",
        "A^{(1)}_t=\\delta_t=-V(x_t)+r_t+\\gamma V(x_{t+1})\n",
        "$$\n",
        "$$\n",
        "A^{(2)}_t=\\delta_t+\\gamma \\delta_{t+1}=-V(x_t)+r_t+\\gamma r_{t+1}+\\gamma^2 V(x_{t+2})\n",
        "$$\n",
        "$$\n",
        "A^{(3)}_t=\\delta_t+\\gamma \\delta_{t+1}+\\gamma^2\\delta_{t+2}=-V(x_t)+r_t+\\gamma r_{t+1}+\\gamma^2 r_{t+2}+\\gamma^2 V(x_{t+3})\n",
        "$$\n",
        "$$\n",
        "\\vdots\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\vdots\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\vdots\n",
        "$$\n",
        "$$\n",
        "A^{(k)}_t=\\sum_{l=0}^{k-1}\\gamma^l\\delta_{t+l}=-V(x_t)+r_t+\\gamma r_{t+1}+\\cdots+\\gamma^{k-1}r_{t+k-1}+\\gamma^k V(x_{t+k})\n",
        "$$\n",
        "GAE is computed as\n",
        "$$\n",
        "A^{GAE}_t=(1-\\lambda)(A^{(1)}_t+\\lambda A^{(2)}_t+\\lambda^2A^{(3)}_t+\\cdots)\n",
        "$$\n",
        "$$\n",
        "=\\sum_{l=0}^{\\infty}(\\gamma\\lambda)^l\\delta_{t+l}\n",
        "$$\n",
        "Here $\\lambda\\in[0,1]$ is another hyperparameter introduced in the calculation of GAE. When $\\lambda=0$, $A^{GAE}_t=\\delta_t=r_t+\\gamma V(x_{t+1})-V(x_t)$ which is the one-step temperal difference estimation of $A_t$. When $\\lambda=1$, $A^{GAE}_t=\\sum_{l=0}^{\\infty}\\gamma^l\\delta_{t+l}=\\sum_{l=0}^{\\infty}\\gamma^lr_{t+l}-V(x_t)$ which is the complete estimation of $A_t$.\n",
        "\n",
        "### Reference\n",
        "\n",
        "[4] Schulman, John, et al. \"High-dimensional continuous control using generalized advantage estimation.\" arXiv preprint arXiv:1506.02438 (2015).\n",
        "https://arxiv.org/abs/1506.02438"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymNjwEPuDtjG"
      },
      "outputs": [],
      "source": [
        "def compute_advantage(gamma, lmbda, rewards, values, next_values, dones):\n",
        "  \"\"\"\n",
        "  This method is an implementation of the Generalized Advantage Estimation (GAE)\n",
        "  Args:\n",
        "    gamma: float\n",
        "    lambda: float\n",
        "    rewards: torch.Tensor of shape (batch_size, 1)\n",
        "    values: torch.Tensor of shape (batch_size, 1)\n",
        "    next_values: torch.Tensor of shape (batch_size, 1)\n",
        "    dones: torch.Tensor of shape (batch_size, 1)\n",
        "  Returns:\n",
        "    advantages: torch.Tensor of shape (batch_size, 1)\n",
        "  \"\"\"\n",
        "  advantages = torch.zeros_like(rewards)\n",
        "  num_steps = advantages.shape[0]\n",
        "  lastgaelam = 0\n",
        "  for t in reversed(range(num_steps)):\n",
        "      if t == num_steps - 1:\n",
        "          nextnonterminal = 1.0 - dones[t]\n",
        "          nextvalues = next_values[t]\n",
        "      else:\n",
        "          nextnonterminal = 1.0 - dones[t + 1]\n",
        "          nextvalues = values[t + 1]\n",
        "      delta = rewards[t] + gamma * nextvalues * nextnonterminal - values[t]\n",
        "      advantages[t] = lastgaelam = delta + gamma * lmbda * nextnonterminal * lastgaelam\n",
        "  return advantages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR6He7ljNR8r"
      },
      "source": [
        "## PPO-Clip\n",
        "\n",
        "The following Python class ``PPOAgent_Clip`` implements the PPO-Clip algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqBt6-F4Mf0W"
      },
      "outputs": [],
      "source": [
        "class PPOAgent_Clip():\n",
        "  def __init__(self, state_dim, action_dim, hidden_dim, gamma, lmbda, clip_coef, vloss_coef, learning_rate, num_epochs, num_minibatches, device, max_grad_norm, ent_coef):\n",
        "    self.state_dim = state_dim\n",
        "    self.action_dim = action_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.gamma = gamma # discount factor\n",
        "    self.lmbda = lmbda # hyperparameter for GAE\n",
        "    self.clip_coef = clip_coef # clip coefficient for PPO\n",
        "    self.vloss_coef = vloss_coef # weight assigned to value loss in the estimation of total loss\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "    self.num_epochs = num_epochs # number of epochs for each trajectory\n",
        "    self.num_minibatches = num_minibatches # number of minibatches for each epoch\n",
        "    self.device = device\n",
        "\n",
        "    self.actor_critic = ActorCritic(state_dim, action_dim, hidden_dim).to(self.device)\n",
        "    self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=learning_rate, eps=1e-5)\n",
        "    self.max_grad_norm = max_grad_norm\n",
        "    self.ent_coef = ent_coef\n",
        "\n",
        "  def set_ent_coef(self, ent_coef):\n",
        "    self.ent_coef = ent_coef\n",
        "\n",
        "  def train(self, train_data):\n",
        "    \"\"\"\n",
        "    states: torch.Tensor with shape (self.batch_size, state_size), a mini-batch of current states\n",
        "    actions: torch.Tensor with shape (self.batch_size, action_size), a mini-batch of current actions\n",
        "    rewards: torch.Tensor with shape (self.batch_size, 1), a mini-batch of rewards\n",
        "    next_states: torch.Tensor with shape (self.batch_size, state_size), a mini-batch of next states\n",
        "    dones: torch.Tensor with shape (self.batch_size, 1), a mini-batch of 0-1 integers,\n",
        "            where 1 means the episode terminates for that sample;\n",
        "                0 means the episode does not terminate for that sample.\n",
        "    \"\"\"\n",
        "    states = torch.tensor(np.array(train_data['states']), dtype=torch.float).reshape(-1, state_dim).to(self.device) #(B, state_size)\n",
        "    actions = torch.tensor(np.array(train_data['actions']), dtype=torch.float).reshape(-1, action_dim).to(self.device) #(B, action_size)\n",
        "    rewards = torch.tensor(np.array(train_data['rewards']), dtype=torch.float).reshape(-1, 1).to(self.device)  #(B, 1)\n",
        "    next_states = torch.tensor(np.array(train_data['next_states']), dtype=torch.float).reshape(-1, state_dim).to(self.device) #(B, state_size)\n",
        "    dones = torch.tensor(np.array(train_data['dones']), dtype=torch.float).reshape(-1, 1).to(self.device) #(B, 1)\n",
        "\n",
        "    values = self.actor_critic.get_value(states).detach().squeeze(1)\n",
        "    next_values = self.actor_critic.get_value(next_states).detach().squeeze(1)\n",
        "    advantages = compute_advantage(self.gamma, self.lmbda, rewards, values, next_values, dones).to(self.device)\n",
        "\n",
        "    advantages = advantages.reshape(-1) # flatten the advantages\n",
        "    values = values.reshape(-1) # flatten the values\n",
        "\n",
        "    old_logprobs, _ = self.actor_critic.get_action_or_logprob(states, actions)\n",
        "    old_logprobs = old_logprobs.detach()\n",
        "\n",
        "    batch_size = states.shape[0]\n",
        "    b_inds = np.arange(batch_size)\n",
        "    for epoch in range(self.num_epochs):\n",
        "      np.random.shuffle(b_inds) # shuffle the index of training data for sampling\n",
        "      minibatch_size = int(batch_size // self.num_minibatches)\n",
        "      for start in range(0, batch_size, minibatch_size):\n",
        "        end = start + minibatch_size\n",
        "        mb_inds = b_inds[start:end] # index of the training data in the minibatch\n",
        "\n",
        "        new_logprobs, entropy = self.actor_critic.get_action_or_logprob(states[mb_inds], actions[mb_inds])\n",
        "        logratio = new_logprobs - old_logprobs[mb_inds]\n",
        "        ratio = logratio.exp()\n",
        "\n",
        "        mb_advantages = advantages[mb_inds]\n",
        "        mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8) # normalize the advantages\n",
        "        policy_loss1 = -mb_advantages * ratio # unclipped loss\n",
        "        policy_loss2 = -mb_advantages * torch.clamp(ratio, 1 - self.clip_coef, 1 + self.clip_coef) # clipped loss\n",
        "        policy_loss = torch.max(policy_loss1, policy_loss2).mean() # policy loss\n",
        "\n",
        "        newvalues = self.actor_critic.get_value(states[mb_inds]).reshape(-1)\n",
        "        returns = values[mb_inds] + advantages[mb_inds] # targets of the value function\n",
        "        value_loss = 0.5 * ((newvalues - returns) ** 2).mean() # value function loss\n",
        "\n",
        "        entropy_loss = entropy.mean()\n",
        "        loss = policy_loss - self.ent_coef * entropy_loss + self.vloss_coef * value_loss # total loss\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.actor_critic.parameters(), self.max_grad_norm)\n",
        "        self.optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kXZwM1OT7kN"
      },
      "source": [
        "## PPO-Penalty\n",
        "The following Python class ``PPOAgent_Penalty`` implements the PPO-Penalty algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ne-pSmiEJLzD"
      },
      "outputs": [],
      "source": [
        "class PPOAgent_Penalty():\n",
        "  def __init__(self, state_dim, action_dim, hidden_dim, gamma, lmbda, vloss_coef, beta, epsilon, learning_rate, num_epochs, num_minibatches, device, max_grad_norm, ent_coef):\n",
        "    self.state_dim = state_dim\n",
        "    self.action_dim = action_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.gamma = gamma # discount factor\n",
        "    self.lmbda = lmbda # hyperparameter for GAE\n",
        "    self.vloss_coef = vloss_coef\n",
        "    self.beta = beta # initialization of beta\n",
        "    self.epsilon = epsilon # hyperparameter used to compare with KL divergence\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "    self.num_epochs = num_epochs\n",
        "    self.num_minibatches = num_minibatches\n",
        "    self.device = device\n",
        "\n",
        "    self.actor_critic = ActorCritic(state_dim, action_dim, hidden_dim).to(self.device)\n",
        "    self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=learning_rate, eps=1e-5)\n",
        "    self.max_grad_norm = max_grad_norm\n",
        "    self.ent_coef = ent_coef\n",
        "\n",
        "  def set_ent_coef(self, ent_coef):\n",
        "    self.ent_coef = ent_coef\n",
        "\n",
        "  def train(self, train_data):\n",
        "    \"\"\"\n",
        "    states: torch.Tensor with shape (self.batch_size, state_size), a mini-batch of current states\n",
        "    actions: torch.Tensor with shape (self.batch_size, action_size), a mini-batch of current actions\n",
        "    rewards: torch.Tensor with shape (self.batch_size, 1), a mini-batch of rewards\n",
        "    next_states: torch.Tensor with shape (self.batch_size, state_size), a mini-batch of next states\n",
        "    dones: torch.Tensor with shape (self.batch_size, 1), a mini-batch of 0-1 integers,\n",
        "            where 1 means the episode terminates for that sample;\n",
        "                0 means the episode does not terminate for that sample.\n",
        "    \"\"\"\n",
        "    states = torch.tensor(np.array(train_data['states']), dtype=torch.float).reshape(-1, state_dim).to(self.device) #(B, state_size)\n",
        "    actions = torch.tensor(np.array(train_data['actions']), dtype=torch.float).reshape(-1, action_dim).to(self.device) #(B, action_size)\n",
        "    rewards = torch.tensor(np.array(train_data['rewards']), dtype=torch.float).reshape(-1, 1).to(self.device)  #(B, 1)\n",
        "    next_states = torch.tensor(np.array(train_data['next_states']), dtype=torch.float).reshape(-1, state_dim).to(self.device) #(B, state_size)\n",
        "    dones = torch.tensor(np.array(train_data['dones']), dtype=torch.float).reshape(-1, 1).to(self.device) #(B, 1)\n",
        "\n",
        "    values = self.actor_critic.get_value(states).detach().squeeze(1)\n",
        "    next_values = self.actor_critic.get_value(next_states).detach().squeeze(1)\n",
        "    advantages = compute_advantage(self.gamma, self.lmbda, rewards, values, next_values, dones).to(self.device)\n",
        "\n",
        "    advantages = advantages.reshape(-1) # flatten the advantages\n",
        "    values = values.reshape(-1) # flatten the values\n",
        "\n",
        "    with torch.no_grad():\n",
        "      old_logprobs, _ = self.actor_critic.get_action_or_logprob(states, actions)\n",
        "      old_logprobs = old_logprobs.detach()\n",
        "\n",
        "      old_means, old_std = self.actor_critic.get_mean_and_std(states)\n",
        "\n",
        "    batch_size = states.shape[0]\n",
        "    b_inds = np.arange(batch_size)\n",
        "    for epoch in range(self.num_epochs):\n",
        "      np.random.shuffle(b_inds)\n",
        "      minibatch_size = int(batch_size // self.num_minibatches)\n",
        "      for start in range(0, batch_size, minibatch_size):\n",
        "        end = start + minibatch_size\n",
        "        mb_inds = b_inds[start:end]\n",
        "\n",
        "        new_logprobs, entropy = self.actor_critic.get_action_or_logprob(states[mb_inds], actions[mb_inds])\n",
        "        logratio = new_logprobs - old_logprobs[mb_inds]\n",
        "        ratio = logratio.exp()\n",
        "\n",
        "        mb_advantages = advantages[mb_inds]\n",
        "        mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8) # normalize the advantages\n",
        "        policy_loss1 = mb_advantages * ratio\n",
        "        old_dists = Normal(old_means[mb_inds], old_std)\n",
        "        new_means, new_std = self.actor_critic.get_mean_and_std(states[mb_inds])\n",
        "        new_dists = Normal(new_means, new_std)\n",
        "        policy_loss2 = torch.distributions.kl.kl_divergence(old_dists, new_dists).sum(1) # estimation of the KL divergence term\n",
        "        policy_loss = -(policy_loss1 - self.beta * policy_loss2).mean() # policy loss\n",
        "\n",
        "        newvalues = self.actor_critic.get_value(states[mb_inds]).reshape(-1)\n",
        "        returns = values[mb_inds] + advantages[mb_inds] # targets of the value function\n",
        "        value_loss = 0.5 * ((newvalues - returns) ** 2).mean() # value function loss\n",
        "\n",
        "        entropy_loss = entropy.mean()\n",
        "        loss = policy_loss - self.ent_coef * entropy_loss + self.vloss_coef * value_loss # total loss\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.actor_critic.parameters(), self.max_grad_norm)\n",
        "        self.optimizer.step()\n",
        "\n",
        "    new_means, new_std = self.actor_critic.get_mean_and_std(states)\n",
        "    new_dists = Normal(new_means, new_std)\n",
        "    old_dists = Normal(old_means, old_std)\n",
        "    kl = torch.distributions.kl.kl_divergence(old_dists, new_dists).sum(1).mean()\n",
        "    if kl >= 1.5*self.epsilon:\n",
        "        self.beta *= 2\n",
        "    if kl <= self.epsilon/1.5:\n",
        "        self.beta *= 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pq34SLeLJbuB"
      },
      "source": [
        "## Online Training Processes of PPO-Clip and Clip-Penalty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C47EK6jjXCwc"
      },
      "outputs": [],
      "source": [
        "# The following code is the online training process of PPO-Clip in the 'Pendulum-v1' gym environment.\n",
        "env_name = 'Pendulum-v1'\n",
        "num_episode = 1000\n",
        "gamma = 0.9\n",
        "lmbda = 0.95\n",
        "clip_coef = 0.05\n",
        "vloss_coef = 0.5\n",
        "learning_rate = 3e-4\n",
        "num_epochs = 10\n",
        "num_minibatches = 4\n",
        "max_grad_norm = 5.0\n",
        "ent_coef = 0.0\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "seed = 0\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "env = gym.make(env_name)\n",
        "env.reset(seed=seed)\n",
        "\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "hidden_dim = 64\n",
        "\n",
        "agent_clip = PPOAgent_Clip(state_dim, action_dim, hidden_dim, gamma, lmbda, clip_coef, vloss_coef, learning_rate, num_epochs, num_minibatches, device, max_grad_norm, ent_coef)\n",
        "\n",
        "episode_return_list = []\n",
        "episode_return_deque = deque(maxlen=100)\n",
        "moving_avg_clip = []\n",
        "for episode in range(num_episode):\n",
        "  episodic_return = 0\n",
        "  timestep_for_cur_episode = 0\n",
        "  train_data = {'states': [], 'actions': [], 'next_states': [], 'rewards': [], 'dones': []}\n",
        "  state = env.reset()\n",
        "  done = False\n",
        "  while not done:\n",
        "    action = agent_clip.actor_critic.get_action_or_logprob(torch.Tensor(state).to(device))\n",
        "    next_state, reward, done, info = env.step(action.cpu().numpy()[0])\n",
        "    train_data['states'].append(state)\n",
        "    train_data['actions'].append(action)\n",
        "    train_data['next_states'].append(next_state)\n",
        "    train_data['rewards'].append(reward)\n",
        "    train_data['dones'].append(done)\n",
        "    episodic_return += reward\n",
        "    timestep_for_cur_episode += 1\n",
        "    if done:\n",
        "      break\n",
        "    state = next_state.reshape(-1)\n",
        "  agent_clip.train(train_data)\n",
        "\n",
        "  episode_return_list.append(episodic_return)\n",
        "  episode_return_deque.append(episodic_return)\n",
        "  print('Ep. {}, Episode Return: {}'.format(episode + 1, episodic_return), end='')\n",
        "\n",
        "  if len(episode_return_deque) == 100:\n",
        "      # Mean of last 100 episodes\n",
        "      avg_reward = sum(episode_return_deque) / 100\n",
        "      moving_avg_clip.append(avg_reward)\n",
        "      print(', Moving Average Reward: {}'.format(avg_reward))\n",
        "  else:\n",
        "      print('')\n",
        "\n",
        "returns_clip = episode_return_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0ivYp2LLX7q"
      },
      "outputs": [],
      "source": [
        "# The following code is the online training process of PPO-Penalty in the 'Pendulum-v1' gym environment.\n",
        "env_name = 'Pendulum-v1'\n",
        "num_episode = 1000\n",
        "gamma = 0.9\n",
        "lmbda = 0.95\n",
        "vloss_coef = 0.5\n",
        "beta = 1\n",
        "epsilon = 0.001\n",
        "learning_rate = 3e-4\n",
        "num_epochs = 10\n",
        "num_minibatches = 4\n",
        "max_grad_norm = 5.0\n",
        "ent_coef = 0.0\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "seed = 0\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "env = gym.make(env_name)\n",
        "env.reset(seed=seed)\n",
        "\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "hidden_dim = 64\n",
        "\n",
        "agent_penalty = PPOAgent_Penalty(state_dim, action_dim, hidden_dim, gamma, lmbda, vloss_coef, beta, epsilon, learning_rate, num_epochs, num_minibatches, device, max_grad_norm, ent_coef)\n",
        "\n",
        "episode_return_list = []\n",
        "episode_return_deque = deque(maxlen=100)\n",
        "moving_avg_penalty = []\n",
        "for episode in range(num_episode):\n",
        "  episodic_return = 0\n",
        "  timestep_for_cur_episode = 0\n",
        "  train_data = {'states': [], 'actions': [], 'next_states': [], 'rewards': [], 'dones': []}\n",
        "  state = env.reset()\n",
        "  done = False\n",
        "  while not done:\n",
        "    action = agent_penalty.actor_critic.get_action_or_logprob(torch.Tensor(state).to(device))\n",
        "    next_state, reward, done, info = env.step(action.cpu().numpy()[0])\n",
        "    train_data['states'].append(state)\n",
        "    train_data['actions'].append(action)\n",
        "    train_data['next_states'].append(next_state)\n",
        "    train_data['rewards'].append(reward)\n",
        "    train_data['dones'].append(done)\n",
        "    episodic_return += reward\n",
        "    timestep_for_cur_episode += 1\n",
        "    if done:\n",
        "      break\n",
        "    state = next_state.reshape(-1)\n",
        "  agent_penalty.train(train_data)\n",
        "\n",
        "  episode_return_list.append(episodic_return)\n",
        "  episode_return_deque.append(episodic_return)\n",
        "  print('Ep. {}, Episode Return: {}'.format(episode + 1, episodic_return), end='')\n",
        "\n",
        "  if len(episode_return_deque) == 100:\n",
        "      # Mean of last 100 episodes\n",
        "      avg_reward = sum(episode_return_deque) / 100\n",
        "      moving_avg_penalty.append(avg_reward)\n",
        "      print(', Moving Average Reward: {}'.format(avg_reward))\n",
        "  else:\n",
        "      print('')\n",
        "\n",
        "returns_penalty = episode_return_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vkoXTbQLDuy"
      },
      "source": [
        "## Comparison of the Performances of PPO-Clip and Clip-Penalty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6j0xqEonPD3y"
      },
      "outputs": [],
      "source": [
        "# Comparison\n",
        "# Plotting reward vs episode for PPO-Clip and PPO-Penalty\n",
        "plt.figure(1)\n",
        "plt.plot(returns_clip, label='PPO-Clip')\n",
        "plt.plot(returns_penalty, label='PPO-Penalty')\n",
        "# plt.title(\"PPO-Clip vs PPO-Penalty\")\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total reward in each episode')\n",
        "plt.legend()\n",
        "\n",
        "# Plotting moving average of reward vs episode for PPO-Clip and PPO-Penalty\n",
        "plt.figure(2)\n",
        "plt.plot(moving_avg_clip, label='PPO-Clip')\n",
        "plt.plot(moving_avg_penalty, label='PPO-Penalty')\n",
        "# plt.title(\"PPO-Clip vs PPO-Penalty\")\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Moving average of reward')\n",
        "plt.legend()\n",
        "\n",
        "# Show the figure\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHWodBFcHcDh"
      },
      "source": [
        "## Evaluation of the Two Policies Trained by PPO-Clip and PPO-Penalty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubqIfyImHwlO"
      },
      "outputs": [],
      "source": [
        "# The following code is the evaluation process of PPO-Clip in the 'Pendulum-v1' gym environment.\n",
        "env_name = 'Pendulum-v1'\n",
        "num_episode = 100\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "seed = 0\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "env = gym.make(env_name)\n",
        "env.reset(seed=seed)\n",
        "\n",
        "episode_return_list = []\n",
        "for episode in range(num_episode):\n",
        "  episodic_return = 0\n",
        "  state = env.reset()\n",
        "  done = False\n",
        "  while not done:\n",
        "    action = agent_clip.actor_critic.get_action_or_logprob(torch.Tensor(state).to(device))\n",
        "    next_state, reward, done, info = env.step(action.cpu().numpy()[0])\n",
        "    episodic_return += reward\n",
        "    if done:\n",
        "      break\n",
        "    state = next_state.reshape(-1)\n",
        "\n",
        "  episode_return_list.append(episodic_return)\n",
        "  print('Ep. {}, Episode Return: {}'.format(episode + 1, episodic_return))\n",
        "\n",
        "returns_clip = episode_return_list\n",
        "avg_return_clip = mean(returns_clip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3dNS6drJJDx"
      },
      "outputs": [],
      "source": [
        "# The following code is the evaluation process of PPO-Penalty in the 'Pendulum-v1' gym environment.\n",
        "env_name = 'Pendulum-v1'\n",
        "num_episode = 100\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "seed = 0\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "env = gym.make(env_name)\n",
        "env.reset(seed=seed)\n",
        "\n",
        "episode_return_list = []\n",
        "for episode in range(num_episode):\n",
        "  episodic_return = 0\n",
        "  state = env.reset()\n",
        "  done = False\n",
        "  while not done:\n",
        "    action = agent_penalty.actor_critic.get_action_or_logprob(torch.Tensor(state).to(device))\n",
        "    next_state, reward, done, info = env.step(action.cpu().numpy()[0])\n",
        "    episodic_return += reward\n",
        "    if done:\n",
        "      break\n",
        "    state = next_state.reshape(-1)\n",
        "\n",
        "  episode_return_list.append(episodic_return)\n",
        "  print('Ep. {}, Episode Return: {}'.format(episode + 1, episodic_return))\n",
        "\n",
        "returns_penalty = episode_return_list\n",
        "avg_return_penalty = mean(returns_penalty)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KW3gQsGMKeBR"
      },
      "outputs": [],
      "source": [
        "print(\"The per-episode return averaged over 100 episode:\")\n",
        "print(\"PPO-Clip:\", avg_return_clip)\n",
        "print(\"PPO-Penalty:\", avg_return_penalty)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rendering the Pendulum Using Two Policies Trained by PPO-Clip and PPO-Penalty"
      ],
      "metadata": {
        "id": "u9Qajyz80SGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install required packages\n",
        "!apt-get update\n",
        "!apt-get install -y xvfb libgtk-3-dev\n",
        "!apt-get autoremove -y\n",
        "!pip install pyvirtualdisplay==2.2\n",
        "!pip install pyppeteer>=0.2.2\n",
        "!pip install pyopengl>=3.1.5\n",
        "!pip install pygame\n",
        "!pyppeteer-install"
      ],
      "metadata": {
        "id": "vG5uhAr2sVdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required packages\n",
        "import matplotlib.pyplot as plt\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay"
      ],
      "metadata": {
        "id": "4EKgmD2P074i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rendering the pendulum using PPO-Clip for 5 episodes\n",
        "display = Display(visible=False, size=(3, 4))\n",
        "_ = display.start()\n",
        "\n",
        "env_name = 'Pendulum-v1'\n",
        "num_episode = 5\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "seed = 0\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "env = gym.make(env_name)\n",
        "env.reset(seed=seed)\n",
        "\n",
        "img = plt.imshow(env.render(mode='rgb_array'))\n",
        "\n",
        "for episode in range(num_episode):\n",
        "  state = env.reset()\n",
        "  done = False\n",
        "  while not done:\n",
        "    img.set_data(env.render(mode='rgb_array'))\n",
        "    ipythondisplay.display(plt.gcf())\n",
        "    ipythondisplay.clear_output(wait=True)\n",
        "\n",
        "    action = agent_clip.actor_critic.get_action_or_logprob(torch.Tensor(state).to(device))\n",
        "    next_state, reward, done, info = env.step(action.cpu().numpy()[0])\n",
        "    if done:\n",
        "      break\n",
        "    state = next_state.reshape(-1)"
      ],
      "metadata": {
        "id": "0RRF7m_q00Ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rendering the pendulum using PPO-Penalty for 5 episodes\n",
        "display = Display(visible=False, size=(3, 4))\n",
        "_ = display.start()\n",
        "\n",
        "env_name = 'Pendulum-v1'\n",
        "num_episode = 5\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "seed = 0\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "env = gym.make(env_name)\n",
        "env.reset(seed=seed)\n",
        "\n",
        "img = plt.imshow(env.render(mode='rgb_array'))\n",
        "\n",
        "for episode in range(num_episode):\n",
        "  state = env.reset()\n",
        "  done = False\n",
        "  while not done:\n",
        "    img.set_data(env.render(mode='rgb_array'))\n",
        "    ipythondisplay.display(plt.gcf())\n",
        "    ipythondisplay.clear_output(wait=True)\n",
        "\n",
        "    action = agent_penalty.actor_critic.get_action_or_logprob(torch.Tensor(state).to(device))\n",
        "    next_state, reward, done, info = env.step(action.cpu().numpy()[0])\n",
        "    if done:\n",
        "      break\n",
        "    state = next_state.reshape(-1)\n",
        "\n",
        "env.close()\n",
        "_ = display.stop()"
      ],
      "metadata": {
        "id": "Vspp9bYO1PlF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}